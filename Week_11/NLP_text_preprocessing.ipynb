{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0f68ae5",
   "metadata": {},
   "source": [
    "# Natural Language Processing - Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2054b14d",
   "metadata": {},
   "source": [
    "## Libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba4eb3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/vscode/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /workspaces/data_analytics/Week_11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/vscode/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Import only once\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.chunk import tree2conlltags\n",
    "from nltk.chunk import conlltags2tree\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Current working directory\n",
    "print('Current working directory:', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e252684a",
   "metadata": {},
   "source": [
    "## Defining documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8057467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The car is driven on the road. The truck is driven on the highway. The bicycle is driven on the bicycle path.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining documents (=sentenses)\n",
    "d1 = 'The car is driven on the road.'\n",
    "d2 = 'The truck is driven on the highway.'\n",
    "d3 = 'The bicycle is driven on the bicycle path.'\n",
    "\n",
    "corpus_01 = d1 + ' ' + d2 + ' ' + d3\n",
    "corpus_01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fadda5",
   "metadata": {},
   "source": [
    "## Text preprocessing\n",
    "#### Steps:\n",
    "- Text to lowercase\n",
    "- Removing punctuations\n",
    "- Tokenization\n",
    "- Removal of stop words\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e649b8",
   "metadata": {},
   "source": [
    "### Text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2666c8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the car is driven on the road. the truck is driven on the highway. the bicycle is driven on the bicycle path.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text to lowercase function\n",
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Text to lowercase\n",
    "corpus_02 = text_lowercase(corpus_01)\n",
    "corpus_02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c837286f",
   "metadata": {},
   "source": [
    "### Removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90067406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the car is driven on the road the truck is driven on the highway the bicycle is driven on the bicycle path'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation function\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "# Remove punctuation\n",
    "corpus_03 = remove_punctuation(corpus_02)\n",
    "corpus_03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986153d2",
   "metadata": {},
   "source": [
    "### Tokenize text & removal of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2e99fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of english stopwords:\n",
      "{'by', 'ain', 'nor', \"wouldn't\", 'should', 'and', 'with', 'being', \"you're\", 'm', 'or', 'will', 'whom', 'mightn', 'yours', 'same', 't', \"don't\", 'don', \"you'd\", \"doesn't\", 'both', 'been', 'all', 'some', 'won', 'only', 'are', 'so', 'each', 'having', \"wasn't\", 'of', 'off', 'have', 'other', 'i', 's', 'their', \"haven't\", 'what', 'y', 'him', 'be', 'below', 'which', 'mustn', 'over', 'shouldn', \"she's\", \"needn't\", 'hers', 'she', 'hadn', 'couldn', 'he', 'did', 'the', 'where', 're', 'your', 'again', 'at', 'yourselves', 'most', 'than', 'himself', \"won't\", 'then', 'doesn', 'here', \"you've\", \"didn't\", 'now', 'ma', 'd', 'needn', 'shan', 'while', 'when', 'very', 'they', 'them', \"hadn't\", 'up', 'll', 'after', \"mustn't\", 'herself', \"couldn't\", 'her', 'theirs', 'an', \"shan't\", \"aren't\", 'wouldn', 'in', \"shouldn't\", 'not', 'me', 'am', 'aren', \"weren't\", 'you', 'doing', 'this', 'had', 'down', 'for', 'his', 'until', 'once', 'such', 'because', 'o', \"isn't\", 'wasn', 'any', 'a', 'into', 'yourself', 'do', 'from', 'if', 'above', 'under', 'own', 'about', 'to', 'just', 'has', 'ours', 'those', 'we', 'didn', 'further', 'as', 'how', 'haven', 'itself', 'myself', \"hasn't\", 'weren', 'it', 'was', 'does', 'ourselves', \"that'll\", 'these', 'no', 'more', 'between', 'on', \"mightn't\", 'who', 'can', 'themselves', 'were', 'through', 'few', 'but', 'our', 'its', 've', \"you'll\", 'isn', 'my', 'against', 'is', 'out', 'hasn', \"it's\", 'that', 'during', 'there', \"should've\", 'before', 'why', 'too'}\n"
     ]
    }
   ],
   "source": [
    "# Show english stopwords\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "print(\"List of english stopwords:\")\n",
    "print(eng_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d83ab939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car', 'driven', 'road', 'truck', 'driven', 'highway', 'bicycle', 'driven', 'bicycle', 'path']"
     ]
    }
   ],
   "source": [
    "# Function for tokenization and the removal of stopwords\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return filtered_text\n",
    " \n",
    "# Remove stopwords\n",
    "corpus_04 = remove_stopwords(corpus_03)\n",
    "print(corpus_04, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad590183",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "410fed5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before lemmatization:\n",
      "['car', 'driven', 'road', 'truck', 'driven', 'highway', 'bicycle', 'driven', 'bicycle', 'path'] \n",
      "\n",
      "After lemmatization:\n",
      "['car', 'drive', 'road', 'truck', 'drive', 'highway', 'bicycle', 'drive', 'bicycle', 'path']"
     ]
    }
   ],
   "source": [
    "# Initialize Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize string function\n",
    "def lemmatize_word(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]\n",
    "    return lemmas\n",
    "\n",
    "# Lemmatize\n",
    "lem = []\n",
    "for i in corpus_04:\n",
    "    lem.append(lemmatize_word(i))\n",
    "\n",
    "# Nested list to list\n",
    "corpus_05 = [' '.join([str(x) for x in lst]) for lst in lem]\n",
    "\n",
    "print('Before lemmatization:')\n",
    "print(corpus_04, '\\n')\n",
    "\n",
    "print('After lemmatization:')\n",
    "print(corpus_05, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71f981f",
   "metadata": {},
   "source": [
    "# 1.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49a42b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The plane is flown in the sky. The helicopter is flown in the sky. The boat is sailed on the sea.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining documents (=sentenses)\n",
    "d1 = 'The plane is flown in the sky.'\n",
    "d2 = 'The helicopter is flown in the sky.'\n",
    "d3 = 'The boat is sailed on the sea.'\n",
    "\n",
    "corpus_01 = d1 + ' ' + d2 + ' ' + d3\n",
    "corpus_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b6d7e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the plane is flown in the sky. the helicopter is flown in the sky. the boat is sailed on the sea.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text to lowercase function\n",
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Text to lowercase\n",
    "corpus_02 = text_lowercase(corpus_01)\n",
    "corpus_02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5435a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the plane is flown in the sky the helicopter is flown in the sky the boat is sailed on the sea'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation function\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "# Remove punctuation\n",
    "corpus_03 = remove_punctuation(corpus_02)\n",
    "corpus_03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97b36bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of english stopwords:\n",
      "{'by', 'ain', 'nor', \"wouldn't\", 'should', 'and', 'with', 'being', \"you're\", 'm', 'or', 'will', 'whom', 'mightn', 'yours', 'same', 't', \"don't\", 'don', \"you'd\", \"doesn't\", 'both', 'been', 'all', 'some', 'won', 'only', 'are', 'so', 'each', 'having', \"wasn't\", 'of', 'off', 'have', 'other', 'i', 's', 'their', \"haven't\", 'what', 'y', 'him', 'be', 'below', 'which', 'mustn', 'over', 'shouldn', \"she's\", \"needn't\", 'hers', 'she', 'hadn', 'couldn', 'he', 'did', 'the', 'where', 're', 'your', 'again', 'at', 'yourselves', 'most', 'than', 'himself', \"won't\", 'then', 'doesn', 'here', \"you've\", \"didn't\", 'now', 'ma', 'd', 'needn', 'shan', 'while', 'when', 'very', 'they', 'them', \"hadn't\", 'up', 'll', 'after', \"mustn't\", 'herself', \"couldn't\", 'her', 'theirs', 'an', \"shan't\", \"aren't\", 'wouldn', 'in', \"shouldn't\", 'not', 'me', 'am', 'aren', \"weren't\", 'you', 'doing', 'this', 'had', 'down', 'for', 'his', 'until', 'once', 'such', 'because', 'o', \"isn't\", 'wasn', 'any', 'a', 'into', 'yourself', 'do', 'from', 'if', 'above', 'under', 'own', 'about', 'to', 'just', 'has', 'ours', 'those', 'we', 'didn', 'further', 'as', 'how', 'haven', 'itself', 'myself', \"hasn't\", 'weren', 'it', 'was', 'does', 'ourselves', \"that'll\", 'these', 'no', 'more', 'between', 'on', \"mightn't\", 'who', 'can', 'themselves', 'were', 'through', 'few', 'but', 'our', 'its', 've', \"you'll\", 'isn', 'my', 'against', 'is', 'out', 'hasn', \"it's\", 'that', 'during', 'there', \"should've\", 'before', 'why', 'too'}\n"
     ]
    }
   ],
   "source": [
    "# Show english stopwords\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "print(\"List of english stopwords:\")\n",
    "print(eng_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e6a52ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plane', 'flown', 'sky', 'helicopter', 'flown', 'sky', 'boat', 'sailed', 'sea']"
     ]
    }
   ],
   "source": [
    "# Function for tokenization and the removal of stopwords\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return filtered_text\n",
    " \n",
    "# Remove stopwords\n",
    "corpus_04 = remove_stopwords(corpus_03)\n",
    "print(corpus_04, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff4498ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before lemmatization:\n",
      "['plane', 'flown', 'sky', 'helicopter', 'flown', 'sky', 'boat', 'sailed', 'sea'] \n",
      "\n",
      "After lemmatization:\n",
      "['plane', 'fly', 'sky', 'helicopter', 'fly', 'sky', 'boat', 'sail', 'sea']"
     ]
    }
   ],
   "source": [
    "# Initialize Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize string function\n",
    "def lemmatize_word(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]\n",
    "    return lemmas\n",
    "\n",
    "# Lemmatize\n",
    "lem = []\n",
    "for i in corpus_04:\n",
    "    lem.append(lemmatize_word(i))\n",
    "\n",
    "# Nested list to list\n",
    "corpus_05 = [' '.join([str(x) for x in lst]) for lst in lem]\n",
    "\n",
    "print('Before lemmatization:')\n",
    "print(corpus_04, '\\n')\n",
    "\n",
    "print('After lemmatization:')\n",
    "print(corpus_05, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9ad6de",
   "metadata": {},
   "source": [
    "## Redefine the text corpus (pre-processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9485c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can't do markdowns anymore I don't know why.\n",
    "# this is 1.c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08a3cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the section ‘Redefine the text corpus (pre-processed)’ adapt the object named ‘corpus’ by hand. For this, use the pre-processed and lemmatized words from your documents.\n",
    "corpus = ['plane fly sky',\n",
    "            'helicopter fly sky',\n",
    "            'boat sail sea']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affcdaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c3c7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix\n",
      "   boat  fly  helicopter  plane  sail  sea  sky\n",
      "0     0    1           0      1     0    0    1\n",
      "1     0    1           1      0     0    0    1\n",
      "2     1    0           0      0     1    1    0\n"
     ]
    }
   ],
   "source": [
    "#Based on the ‘corpus’, create a document-term matrix with ngram_range=(1,1)\n",
    "# Vectorizer with ngram_range=(1,1)\n",
    "vectorizer = CountVectorizer(min_df=0.0, ngram_range=(1,1))\n",
    "\n",
    "# Transform \n",
    "count = vectorizer.fit_transform(corpus)\n",
    " \n",
    "# Create dataframe\n",
    "df_count = pd.DataFrame(count.toarray(),\n",
    "                        columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print('Document-term matrix')\n",
    "print(df_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a31e4f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix\n",
      "   boat sail  fly sky  helicopter fly  plane fly  sail sea\n",
      "0          0        1               0          1         0\n",
      "1          0        1               1          0         0\n",
      "2          1        0               0          0         1\n"
     ]
    }
   ],
   "source": [
    "# and a document-term matrix with ngram_range=(2,2).\n",
    "# Vectorizer with with ngram_range=(2,2)\n",
    "vectorizer = CountVectorizer(min_df=0.0, ngram_range=(2,2))\n",
    "\n",
    "# Transform \n",
    "count = vectorizer.fit_transform(corpus)\n",
    " \n",
    "# Create dataframe\n",
    "df_count = pd.DataFrame(count.toarray(),\n",
    "                        columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print('Document-term matrix')\n",
    "print(df_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44d8ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae872c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the corpus: 7 \n",
      "\n",
      "The words in the corpus: \n",
      " {'fly', 'boat', 'sky', 'plane', 'helicopter', 'sea', 'sail'}\n",
      "\n",
      "Term Frequency (TF):\n",
      "      fly    boat     sky   plane  helicopter     sea    sail\n",
      "0  0.3333  0.0000  0.3333  0.3333      0.0000  0.0000  0.0000\n",
      "1  0.3333  0.0000  0.3333  0.0000      0.3333  0.0000  0.0000\n",
      "2  0.0000  0.3333  0.0000  0.0000      0.0000  0.3333  0.3333\n"
     ]
    }
   ],
   "source": [
    "# create a Term Frequency (TF) matrix\n",
    "# Compute Term Frequency (TF)\n",
    "words_set = set()\n",
    "for doc in corpus:\n",
    "    words = doc.split(' ')\n",
    "    words_set = words_set.union(set(words))\n",
    "    \n",
    "print('Number of words in the corpus:',len(words_set), '\\n')\n",
    "print('The words in the corpus: \\n', words_set)\n",
    "\n",
    "# Number of documents in the corpus\n",
    "n_docs = len(corpus)\n",
    "\n",
    "# Number of unique words in the corpus \n",
    "n_words_set = len(words_set)\n",
    "\n",
    "df_tf = pd.DataFrame(np.zeros((n_docs, n_words_set)), \n",
    "                     columns=list(words_set))\n",
    "\n",
    "print(\"\\nTerm Frequency (TF):\")\n",
    "for i in range(n_docs):\n",
    "    # Words in the document\n",
    "    words = corpus[i].split(' ')\n",
    "    for w in words:\n",
    "        df_tf[w][i] = df_tf[w][i] + (1 / len(words))\n",
    "        \n",
    "print(df_tf.round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b2d69d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inverse Document Frequency (IDF):\n",
      "            fly:     0.1761\n",
      "           boat:     0.4771\n",
      "            sky:     0.1761\n",
      "          plane:     0.4771\n",
      "     helicopter:     0.4771\n",
      "            sea:     0.4771\n",
      "           sail:     0.4771\n"
     ]
    }
   ],
   "source": [
    "# create a Inverse Document Frequency (IDF) matrix\n",
    "# Computing Inverse Document Frequency (IDF)\n",
    "print(\"\\nInverse Document Frequency (IDF):\")\n",
    "\n",
    "idf = {}\n",
    "\n",
    "for w in words_set:\n",
    "    \n",
    "    # k = number of documents that contain this word\n",
    "    k = 0\n",
    "    \n",
    "    for i in range(n_docs):\n",
    "        if w in corpus[i].split():\n",
    "            k += 1\n",
    "            \n",
    "    idf[w] =  np.log10(n_docs / k).round(4)\n",
    "    \n",
    "    print(f'{w:>15}: {idf[w]:>10}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "833ba99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF:\n",
      "      fly   boat     sky  plane  helicopter    sea   sail\n",
      "0  0.0587  0.000  0.0587  0.159       0.000  0.000  0.000\n",
      "1  0.0587  0.000  0.0587  0.000       0.159  0.000  0.000\n",
      "2  0.0000  0.159  0.0000  0.000       0.000  0.159  0.159\n"
     ]
    }
   ],
   "source": [
    "# create a Term Frequency - Inverse Document Frequency (TF-IDF) matrix\n",
    "# Computing TF-IDF\n",
    "df_tf_idf = df_tf.copy()\n",
    "\n",
    "for w in words_set:\n",
    "    for i in range(n_docs):\n",
    "        df_tf_idf[w][i] = df_tf[w][i] * idf[w]\n",
    "\n",
    "print('\\nTF-IDF:')\n",
    "print(df_tf_idf.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198cc6d0",
   "metadata": {},
   "source": [
    "## Document-term matrix with ngram_range=(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ead679d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix\n",
      "   bicycle  car  drive  highway  path  road  truck\n",
      "0        0    1      1        0     0     1      0\n",
      "1        0    0      1        1     0     0      1\n",
      "2        2    0      1        0     1     0      0\n"
     ]
    }
   ],
   "source": [
    "# Vectorizer with ngram_range=(1,1)\n",
    "vectorizer = CountVectorizer(min_df=0.0, ngram_range=(1,1))\n",
    "\n",
    "# Transform \n",
    "count = vectorizer.fit_transform(corpus)\n",
    " \n",
    "# Create dataframe\n",
    "df_count = pd.DataFrame(count.toarray(),\n",
    "                        columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print('Document-term matrix')\n",
    "print(df_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417feb3a",
   "metadata": {},
   "source": [
    "## Document-term matrix with ngram_range=(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4eb33ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix\n",
      "   bicycle drive  bicycle path  car drive  drive bicycle  drive highway  \\\n",
      "0              0             0          1              0              0   \n",
      "1              0             0          0              0              1   \n",
      "2              1             1          0              1              0   \n",
      "\n",
      "   drive road  truck drive  \n",
      "0           1            0  \n",
      "1           0            1  \n",
      "2           0            0  \n"
     ]
    }
   ],
   "source": [
    "# Vectorizer with with ngram_range=(2,2)\n",
    "vectorizer = CountVectorizer(min_df=0.0, ngram_range=(2,2))\n",
    "\n",
    "# Transform \n",
    "count = vectorizer.fit_transform(corpus)\n",
    " \n",
    "# Create dataframe\n",
    "df_count = pd.DataFrame(count.toarray(),\n",
    "                        columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print('Document-term matrix')\n",
    "print(df_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c846a236",
   "metadata": {},
   "source": [
    "## Term frequency-inverse document frequency (TF-IDF)\n",
    "- For details see: https://www.learndatasci.com/glossary/tf-idf-term-frequency-inverse-document-frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5854fa81",
   "metadata": {},
   "source": [
    "### Term Frequency (TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9ff38f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the corpus: 7 \n",
      "\n",
      "The words in the corpus: \n",
      " {'car', 'path', 'bicycle', 'drive', 'truck', 'road', 'highway'}\n",
      "\n",
      "Term Frequency (TF):\n",
      "      car  path  bicycle   drive   truck    road  highway\n",
      "0  0.3333  0.00      0.0  0.3333  0.0000  0.3333   0.0000\n",
      "1  0.0000  0.00      0.0  0.3333  0.3333  0.0000   0.3333\n",
      "2  0.0000  0.25      0.5  0.2500  0.0000  0.0000   0.0000\n"
     ]
    }
   ],
   "source": [
    "# Compute Term Frequency (TF)\n",
    "words_set = set()\n",
    "for doc in corpus:\n",
    "    words = doc.split(' ')\n",
    "    words_set = words_set.union(set(words))\n",
    "    \n",
    "print('Number of words in the corpus:',len(words_set), '\\n')\n",
    "print('The words in the corpus: \\n', words_set)\n",
    "\n",
    "# Number of documents in the corpus\n",
    "n_docs = len(corpus)\n",
    "\n",
    "# Number of unique words in the corpus \n",
    "n_words_set = len(words_set)\n",
    "\n",
    "df_tf = pd.DataFrame(np.zeros((n_docs, n_words_set)), \n",
    "                     columns=list(words_set))\n",
    "\n",
    "print(\"\\nTerm Frequency (TF):\")\n",
    "for i in range(n_docs):\n",
    "    # Words in the document\n",
    "    words = corpus[i].split(' ')\n",
    "    for w in words:\n",
    "        df_tf[w][i] = df_tf[w][i] + (1 / len(words))\n",
    "        \n",
    "print(df_tf.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91dae3d",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency (IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fe31336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inverse Document Frequency (IDF):\n",
      "            car:     0.4771\n",
      "           path:     0.4771\n",
      "        bicycle:     0.4771\n",
      "          drive:        0.0\n",
      "          truck:     0.4771\n",
      "           road:     0.4771\n",
      "        highway:     0.4771\n"
     ]
    }
   ],
   "source": [
    "# Computing Inverse Document Frequency (IDF)\n",
    "print(\"\\nInverse Document Frequency (IDF):\")\n",
    "\n",
    "idf = {}\n",
    "\n",
    "for w in words_set:\n",
    "    \n",
    "    # k = number of documents that contain this word\n",
    "    k = 0\n",
    "    \n",
    "    for i in range(n_docs):\n",
    "        if w in corpus[i].split():\n",
    "            k += 1\n",
    "            \n",
    "    idf[w] =  np.log10(n_docs / k).round(4)\n",
    "    \n",
    "    print(f'{w:>15}: {idf[w]:>10}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc493eae",
   "metadata": {},
   "source": [
    "### Term Frequency - Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c5ae575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF:\n",
      "     car    path  bicycle  drive  truck   road  highway\n",
      "0  0.159  0.0000   0.0000    0.0  0.000  0.159    0.000\n",
      "1  0.000  0.0000   0.0000    0.0  0.159  0.000    0.159\n",
      "2  0.000  0.1193   0.2386    0.0  0.000  0.000    0.000\n"
     ]
    }
   ],
   "source": [
    "# Computing TF-IDF\n",
    "df_tf_idf = df_tf.copy()\n",
    "\n",
    "for w in words_set:\n",
    "    for i in range(n_docs):\n",
    "        df_tf_idf[w][i] = df_tf[w][i] * idf[w]\n",
    "\n",
    "print('\\nTF-IDF:')\n",
    "print(df_tf_idf.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7b0f38",
   "metadata": {},
   "source": [
    "## Part-of-Speach (POS) tagging\n",
    "For meaning of POS-tags see: https://pythonexamples.org/nltk-pos-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a038ac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5c8c05c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NASA', 'NNP', 'O'),\n",
      " (\"'s\", 'POS', 'O'),\n",
      " ('James', 'NNP', 'O'),\n",
      " ('Webb', 'NNP', 'O'),\n",
      " ('Space', 'NNP', 'O'),\n",
      " ('Telescope', 'NNP', 'O'),\n",
      " ('captured', 'VBD', 'O'),\n",
      " ('stunning', 'JJ', 'O'),\n",
      " ('images', 'NNS', 'O'),\n",
      " ('of', 'IN', 'O'),\n",
      " ('a', 'DT', 'B-NP'),\n",
      " ('distant', 'JJ', 'I-NP'),\n",
      " ('galaxy', 'NN', 'I-NP'),\n",
      " (',', ',', 'O'),\n",
      " ('providing', 'VBG', 'O'),\n",
      " ('new', 'JJ', 'O'),\n",
      " ('insights', 'NNS', 'O'),\n",
      " ('into', 'IN', 'O'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('early', 'JJ', 'I-NP'),\n",
      " ('universe', 'NN', 'I-NP'),\n",
      " ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "text = '''NASA's James Webb Space Telescope captured stunning images of a distant galaxy, providing new insights into the early universe.'''\n",
    "\n",
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent\n",
    "\n",
    "sent = preprocess(text)\n",
    "pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "\n",
    "cp = nltk.RegexpParser(pattern)\n",
    "cs = cp.parse(sent)\n",
    "\n",
    "iob_tagged = tree2conlltags(cs)\n",
    "\n",
    "# Print the POS-tags\n",
    "pprint(iob_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e0b743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 POS-tags\n",
    "\n",
    "# JJ = Adjective\n",
    "# NN = Noun, singular or mass\n",
    "# DT = Determiner\n",
    "# IN = Preposition or subordinating conjunction\n",
    "# VBD = Verb, past tense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd5f231",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''It is a truth universally acknowledged, that a '''\n",
    "\n",
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent\n",
    "\n",
    "sent = preprocess(text)\n",
    "pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "\n",
    "cp = nltk.RegexpParser(pattern)\n",
    "cs = cp.parse(sent)\n",
    "\n",
    "iob_tagged = tree2conlltags(cs)\n",
    "\n",
    "# Print the POS-tags\n",
    "pprint(iob_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1243de",
   "metadata": {},
   "source": [
    "### Jupyter notebook --footer info-- (please always provide this at the end of each submitted notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "017357b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "POSIX\n",
      "Linux | 6.5.0-1025-azure\n",
      "Datetime: 2024-11-27 21:08:00\n",
      "Python Version: 3.11.10\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import socket\n",
    "from platform import python_version\n",
    "from datetime import datetime\n",
    "\n",
    "print('-----------------------------------')\n",
    "print(os.name.upper())\n",
    "print(platform.system(), '|', platform.release())\n",
    "print('Datetime:', datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print('Python Version:', python_version())\n",
    "print('-----------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
